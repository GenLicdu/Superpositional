{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.108302</td>\n",
       "      <td>-0.535908</td>\n",
       "      <td>-0.351269</td>\n",
       "      <td>0.108302</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230777</td>\n",
       "      <td>-0.907482</td>\n",
       "      <td>-0.535908</td>\n",
       "      <td>0.230777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.330796</td>\n",
       "      <td>-0.351269</td>\n",
       "      <td>-0.907482</td>\n",
       "      <td>-0.330796</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.696206</td>\n",
       "      <td>0.233532</td>\n",
       "      <td>-0.851933</td>\n",
       "      <td>0.696206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.133325</td>\n",
       "      <td>-0.582460</td>\n",
       "      <td>0.233532</td>\n",
       "      <td>-0.133325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.666040</td>\n",
       "      <td>-0.851933</td>\n",
       "      <td>-0.582460</td>\n",
       "      <td>-0.666040</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738704</td>\n",
       "      <td>-0.234767</td>\n",
       "      <td>-0.529127</td>\n",
       "      <td>0.738704</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.133308</td>\n",
       "      <td>-0.693722</td>\n",
       "      <td>-0.234767</td>\n",
       "      <td>-0.133308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.600907</td>\n",
       "      <td>-0.529127</td>\n",
       "      <td>-0.693722</td>\n",
       "      <td>-0.600907</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.786672</td>\n",
       "      <td>-0.842429</td>\n",
       "      <td>-0.000359</td>\n",
       "      <td>0.786672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.488673</td>\n",
       "      <td>-0.533993</td>\n",
       "      <td>-0.842429</td>\n",
       "      <td>-0.488673</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.474209</td>\n",
       "      <td>-0.000359</td>\n",
       "      <td>-0.533993</td>\n",
       "      <td>-0.474209</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.097516</td>\n",
       "      <td>-0.956668</td>\n",
       "      <td>0.673904</td>\n",
       "      <td>0.097516</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.621298</td>\n",
       "      <td>-0.956668</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.672272</td>\n",
       "      <td>0.673904</td>\n",
       "      <td>-0.621298</td>\n",
       "      <td>-0.672272</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4    5         6         7  \\\n",
       "0  1.0  0.108302 -0.535908 -0.351269  0.108302  1.0  0.230777 -0.907482   \n",
       "1  1.0  0.696206  0.233532 -0.851933  0.696206  1.0 -0.133325 -0.582460   \n",
       "2  1.0  0.738704 -0.234767 -0.529127  0.738704  1.0 -0.133308 -0.693722   \n",
       "3  1.0  0.786672 -0.842429 -0.000359  0.786672  1.0 -0.488673 -0.533993   \n",
       "4  1.0  0.097516 -0.956668  0.673904  0.097516  1.0 -0.161021 -0.621298   \n",
       "\n",
       "          8         9   10        11        12        13        14   15  \n",
       "0 -0.535908  0.230777  1.0 -0.330796 -0.351269 -0.907482 -0.330796  1.0  \n",
       "1  0.233532 -0.133325  1.0 -0.666040 -0.851933 -0.582460 -0.666040  1.0  \n",
       "2 -0.234767 -0.133308  1.0 -0.600907 -0.529127 -0.693722 -0.600907  1.0  \n",
       "3 -0.842429 -0.488673  1.0 -0.474209 -0.000359 -0.533993 -0.474209  1.0  \n",
       "4 -0.956668 -0.161021  1.0 -0.672272  0.673904 -0.621298 -0.672272  1.0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dynamicgraph.csv').iloc[:,1:-1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scale(pd.read_csv('Dynamicgraph.csv').iloc[:,1:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 16)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    for j in range(df.shape[1]):\n",
    "        if df[i][j] == 0:\n",
    "            df[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(df.reshape(df.shape[0],4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # 替换NaN值为均值\n",
    "    nan_mask = np.isnan(data)\n",
    "    data[nan_mask] = np.nanmean(data)\n",
    "    \n",
    "    # 或者替换为特定的值，例如 0\n",
    "    # data[nan_mask] = 0  # 替换为0\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.17714915, 0.36213489, 0.42320646],\n",
       "       [0.17714915, 1.        , 0.52156084, 0.74787603],\n",
       "       [0.36213489, 0.52156084, 1.        , 1.39719796],\n",
       "       [0.42320646, 0.74787603, 1.39719796, 1.        ]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    E = []\n",
    "    for i in range(X.shape[0]):\n",
    "        P = []\n",
    "        for j in range(X.shape[1]):\n",
    "            if i !=j:\n",
    "                e = -X[i][j]*np.log(X[i][j])\n",
    "                P.append(e)\n",
    "        P = np.array(P)\n",
    "        E.append(np.sum(P))\n",
    "    return np.array(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphentropy(X):\n",
    "    E = []\n",
    "    for i in range(X.shape[0]):\n",
    "        e = entropy(X[i])\n",
    "        E.append(np.sum(e))\n",
    "    return np.array(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = graphentropy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inf in entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = cdist(entropies.reshape(-1, 1), entropies.reshape(-1, 1), metric='euclidean')\n",
    "distances.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_indices = np.argmin(distances, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199], dtype=int64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=16):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "# Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, bottleneck_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 16\n",
    "embed_dim = 16\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 3\n",
    "hidden_dim = 32\n",
    "bottleneck_dim = 16\n",
    "lr = 0.01\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "transformer_encoder = TransformerEncoder(input_dim, embed_dim, num_heads, ff_dim, num_layers)\n",
    "autoencoder = Autoencoder(input_dim, hidden_dim, bottleneck_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(transformer_encoder.parameters()) + list(autoencoder.parameters()), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.50752799, 0.48567148, 1.31222442],\n",
       "       [0.50752799, 1.        , 1.03814172, 1.08521685],\n",
       "       [0.48567148, 1.03814172, 1.        , 0.76051659],\n",
       "       [1.31222442, 1.08521685, 0.76051659, 1.        ]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0054, 1.2586, 1.6396, 1.0054, 1.0000, 0.2559, 0.4589, 1.2586,\n",
       "         0.2559, 1.0000, 0.1213, 1.6396, 0.4589, 0.1213, 1.0000]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(data[2].flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "similar_x = torch.tensor(data[most_similar_indices[2]].flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "similar_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.        , 1.0054204 , 1.2586383 , 1.63960144],\n",
       "        [1.0054204 , 1.        , 0.25585827, 0.45890966],\n",
       "        [1.2586383 , 0.25585827, 1.        , 0.12129512],\n",
       "        [1.63960144, 0.45890966, 0.12129512, 1.        ]]),\n",
       " array([[1.        , 0.17714915, 0.36213489, 0.42320646],\n",
       "        [0.17714915, 1.        , 0.52156084, 0.74787603],\n",
       "        [0.36213489, 0.52156084, 1.        , 1.39719796],\n",
       "        [0.42320646, 0.74787603, 1.39719796, 1.        ]]),\n",
       " 2)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[most_similar_indices[2]],data[5],most_similar_indices[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 16])) that is different to the input size (torch.Size([1, 1, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.33832161225378515\n",
      "Epoch 2/100, Loss: 0.25754885010421275\n",
      "Epoch 3/100, Loss: 0.22625023159198462\n",
      "Epoch 4/100, Loss: 0.22103169823996724\n",
      "Epoch 5/100, Loss: 0.21953314787708222\n",
      "Epoch 6/100, Loss: 0.21266552071087064\n",
      "Epoch 7/100, Loss: 0.21086209332104772\n",
      "Epoch 8/100, Loss: 0.2099779650522396\n",
      "Epoch 9/100, Loss: 0.20685343314427881\n",
      "Epoch 10/100, Loss: 0.21016464163083584\n",
      "Epoch 11/100, Loss: 0.213230221667327\n",
      "Epoch 12/100, Loss: 0.20689185524825007\n",
      "Epoch 13/100, Loss: 0.20251080514863135\n",
      "Epoch 14/100, Loss: 0.20649932875763624\n",
      "Epoch 15/100, Loss: 0.20571111652068794\n",
      "Epoch 16/100, Loss: 0.21376956368796526\n",
      "Epoch 17/100, Loss: 0.20311998409219087\n",
      "Epoch 18/100, Loss: 0.20837560452520848\n",
      "Epoch 19/100, Loss: 0.20967716386541724\n",
      "Epoch 20/100, Loss: 0.2083749811211601\n",
      "Epoch 21/100, Loss: 0.20465779855847357\n",
      "Epoch 22/100, Loss: 0.2067534336913377\n",
      "Epoch 23/100, Loss: 0.20763037267606704\n",
      "Epoch 24/100, Loss: 0.20826562562026082\n",
      "Epoch 25/100, Loss: 0.20598248824477194\n",
      "Epoch 26/100, Loss: 0.20231668977532535\n",
      "Epoch 27/100, Loss: 0.20540275680832565\n",
      "Epoch 28/100, Loss: 0.1996377917844802\n",
      "Epoch 29/100, Loss: 0.2165872597321868\n",
      "Epoch 30/100, Loss: 0.20261684177443384\n",
      "Epoch 31/100, Loss: 0.2033788048196584\n",
      "Epoch 32/100, Loss: 0.20468103673309088\n",
      "Epoch 33/100, Loss: 0.21734950261190533\n",
      "Epoch 34/100, Loss: 0.20094035120215267\n",
      "Epoch 35/100, Loss: 0.203283277656883\n",
      "Epoch 36/100, Loss: 0.19782283204142004\n",
      "Epoch 37/100, Loss: 0.20262987380381672\n",
      "Epoch 38/100, Loss: 0.19977954423520713\n",
      "Epoch 39/100, Loss: 0.203621214996092\n",
      "Epoch 40/100, Loss: 0.1972928470140323\n",
      "Epoch 41/100, Loss: 0.20011499680113048\n",
      "Epoch 42/100, Loss: 0.19784757474903017\n",
      "Epoch 43/100, Loss: 0.205138217555359\n",
      "Epoch 44/100, Loss: 0.20138600843958557\n",
      "Epoch 45/100, Loss: 0.2095303045772016\n",
      "Epoch 46/100, Loss: 0.2019364365050569\n",
      "Epoch 47/100, Loss: 0.2067980967881158\n",
      "Epoch 48/100, Loss: 0.1982263103686273\n",
      "Epoch 49/100, Loss: 0.20710671486333013\n",
      "Epoch 50/100, Loss: 0.20818696431815625\n",
      "Epoch 51/100, Loss: 0.19903658483643086\n",
      "Epoch 52/100, Loss: 0.20892552289180458\n",
      "Epoch 53/100, Loss: 0.19670754591468723\n",
      "Epoch 54/100, Loss: 0.19443606766406446\n",
      "Epoch 55/100, Loss: 0.1928773282514885\n",
      "Epoch 56/100, Loss: 0.1938169900421053\n",
      "Epoch 57/100, Loss: 0.196375262029469\n",
      "Epoch 58/100, Loss: 0.1924099339125678\n",
      "Epoch 59/100, Loss: 0.20021797937341035\n",
      "Epoch 60/100, Loss: 0.19882695740554482\n",
      "Epoch 61/100, Loss: 0.19652711163274944\n",
      "Epoch 62/100, Loss: 0.19277808257378637\n",
      "Epoch 63/100, Loss: 0.19302088228520006\n",
      "Epoch 64/100, Loss: 0.19375920313876122\n",
      "Epoch 65/100, Loss: 0.1918416179297492\n",
      "Epoch 66/100, Loss: 0.19509635074064136\n",
      "Epoch 67/100, Loss: 0.19350462595932186\n",
      "Epoch 68/100, Loss: 0.20094579270575197\n",
      "Epoch 69/100, Loss: 0.20695644032210112\n",
      "Epoch 70/100, Loss: 0.19603120202664287\n",
      "Epoch 71/100, Loss: 0.19405503349378705\n",
      "Epoch 72/100, Loss: 0.19497080736327915\n",
      "Epoch 73/100, Loss: 0.1972331312811002\n",
      "Epoch 74/100, Loss: 0.19135038735810667\n",
      "Epoch 75/100, Loss: 0.20191397357266397\n",
      "Epoch 76/100, Loss: 0.20359727257862686\n",
      "Epoch 77/100, Loss: 0.19490524929016828\n",
      "Epoch 78/100, Loss: 0.19570613600779324\n",
      "Epoch 79/100, Loss: 0.19399345612619073\n",
      "Epoch 80/100, Loss: 0.19011227833572775\n",
      "Epoch 81/100, Loss: 0.1936917420849204\n",
      "Epoch 82/100, Loss: 0.19879851098638027\n",
      "Epoch 83/100, Loss: 0.19314509390853346\n",
      "Epoch 84/100, Loss: 0.19231727368663998\n",
      "Epoch 85/100, Loss: 0.193068007491529\n",
      "Epoch 86/100, Loss: 0.19422448719386012\n",
      "Epoch 87/100, Loss: 0.19145293002948166\n",
      "Epoch 88/100, Loss: 0.189931162847206\n",
      "Epoch 89/100, Loss: 0.19717778683640064\n",
      "Epoch 90/100, Loss: 0.19311735320370643\n",
      "Epoch 91/100, Loss: 0.19233283734414727\n",
      "Epoch 92/100, Loss: 0.19338694281410426\n",
      "Epoch 93/100, Loss: 0.19848286387044936\n",
      "Epoch 94/100, Loss: 0.20055921788793057\n",
      "Epoch 95/100, Loss: 0.20367437213659287\n",
      "Epoch 96/100, Loss: 0.19801372973248363\n",
      "Epoch 97/100, Loss: 0.20747494080103934\n",
      "Epoch 98/100, Loss: 0.2055298722954467\n",
      "Epoch 99/100, Loss: 0.20167249790392816\n",
      "Epoch 100/100, Loss: 0.1949527994263917\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(data)):\n",
    "        x = torch.tensor(data[i].flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "        similar_x = torch.tensor(data[most_similar_indices[i]].flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        transformer_output = transformer_encoder(x)\n",
    "\n",
    "        # Autoencoder\n",
    "        encoded, decoded = autoencoder(similar_x)\n",
    "\n",
    "        # 损失计算\n",
    "        loss1 = mse_loss(transformer_output, encoded)\n",
    "        loss2 = mse_loss(similar_x, decoded)\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, data):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data)):\n",
    "            x = torch.tensor(data[i].flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "            embedding = model(x)\n",
    "            embeddings.append(embedding.squeeze(0).numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 获取嵌入向量\n",
    "embeddings = get_embeddings(transformer_encoder, data)\n",
    "#np.savetxt('Embedding_vector.csv', embeddings, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "np.save('embeddings.npy', embeddings)\n",
    "\n",
    "print(\"Embeddings saved to embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
